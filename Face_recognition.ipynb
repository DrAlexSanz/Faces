{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face recognition",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrAlexSanz/Faces/blob/master/Face_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Xn0gRunxhO",
        "colab_type": "text"
      },
      "source": [
        "Welcome to the first assignment of week 4! Here you will build a face recognition system. Many of the ideas presented here are from FaceNet. In lecture, we also talked about DeepFace.\n",
        "\n",
        "**Face recognition** problems commonly fall into two categories:\n",
        "\n",
        "**Face Verification** - \"is this the claimed person?\". For example, at some airports, you can pass through customs by letting a system scan your passport and then verifying that you (the person carrying the passport) are the correct person. A mobile phone that unlocks using your face is also using face verification. This is a 1:1 matching problem.\n",
        "Face Recognition - \"who is this person?\". For example, the video lecture showed a face recognition ([video](https://www.youtube.com/watch?v=wr4rx0Spihs)) of Baidu employees entering the office without needing to otherwise identify themselves. This is a 1:K matching problem.\n",
        "FaceNet learns a neural network that encodes a face image into a vector of 128 numbers. By comparing two such vectors, you can then determine if two pictures are of the same person.\n",
        "\n",
        "In this assignment, you will:\n",
        "\n",
        "Implement the triplet loss function\n",
        "Use a pretrained model to map face images into 128-dimensional encodings\n",
        "Use these encodings to perform face verification and face recognition\n",
        "In this exercise, we will be using a pre-trained model which represents ConvNet activations using a \"channels first\" convention, as opposed to the \"channels last\" convention used in lecture and previous programming assignments. In other words, a batch of images will be of shape $(m, n_C, n_H, n_W)$ instead of $(m, n_H, n_W, n_C)$. Both of these conventions have a reasonable amount of traction among open-source implementations; there isn't a uniform standard yet within the deep learning community.\n",
        "\n",
        "Let's load the required packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK1E6Hhyl3Tn",
        "colab_type": "code",
        "outputId": "ef8ff7f7-f8e4-430a-b68b-e0ae2602db3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = [18, 12]\n",
        "import h5py\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from keras import layers, optimizers\n",
        "from keras.layers import Input, Dense, Conv2D, Activation, ZeroPadding2D, BatchNormalization, Flatten, Add\n",
        "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
        "\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers.core import Lambda, Flatten, Dense\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.engine.topology import Layer\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "\n",
        "from keras.preprocessing import image\n",
        "\n",
        "from keras.utils import layer_utils, plot_model, to_categorical\n",
        "\n",
        "from keras.callbacks import History, ModelCheckpoint\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"Everything imported correctly\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Everything imported correctly\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9T431cZv-9i",
        "colab_type": "code",
        "outputId": "6e95d532-197c-4da2-c4e9-59058dc41712",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!rm -rf Faces\n",
        "\n",
        "!git clone https://github.com/DrAlexSanz/Faces.git"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Faces'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/85)\u001b[K\rremote: Counting objects:   2% (2/85)\u001b[K\rremote: Counting objects:   3% (3/85)\u001b[K\rremote: Counting objects:   4% (4/85)\u001b[K\rremote: Counting objects:   5% (5/85)\u001b[K\rremote: Counting objects:   7% (6/85)\u001b[K\rremote: Counting objects:   8% (7/85)\u001b[K\rremote: Counting objects:   9% (8/85)\u001b[K\rremote: Counting objects:  10% (9/85)\u001b[K\rremote: Counting objects:  11% (10/85)\u001b[K\rremote: Counting objects:  12% (11/85)\u001b[K\rremote: Counting objects:  14% (12/85)\u001b[K\rremote: Counting objects:  15% (13/85)\u001b[K\rremote: Counting objects:  16% (14/85)\u001b[K\rremote: Counting objects:  17% (15/85)\u001b[K\rremote: Counting objects:  18% (16/85)\u001b[K\rremote: Counting objects:  20% (17/85)\u001b[K\rremote: Counting objects:  21% (18/85)\u001b[K\rremote: Counting objects:  22% (19/85)\u001b[K\rremote: Counting objects:  23% (20/85)\u001b[K\rremote: Counting objects:  24% (21/85)\u001b[K\rremote: Counting objects:  25% (22/85)\u001b[K\rremote: Counting objects:  27% (23/85)\u001b[K\rremote: Counting objects:  28% (24/85)\u001b[K\rremote: Counting objects:  29% (25/85)\u001b[K\rremote: Counting objects:  30% (26/85)\u001b[K\rremote: Counting objects:  31% (27/85)\u001b[K\rremote: Counting objects:  32% (28/85)\u001b[K\rremote: Counting objects:  34% (29/85)\u001b[K\rremote: Counting objects:  35% (30/85)\u001b[K\rremote: Counting objects:  36% (31/85)\u001b[K\rremote: Counting objects:  37% (32/85)\u001b[K\rremote: Counting objects:  38% (33/85)\u001b[K\rremote: Counting objects:  40% (34/85)\u001b[K\rremote: Counting objects:  41% (35/85)\u001b[K\rremote: Counting objects:  42% (36/85)\u001b[K\rremote: Counting objects:  43% (37/85)\u001b[K\rremote: Counting objects:  44% (38/85)\u001b[K\rremote: Counting objects:  45% (39/85)\u001b[K\rremote: Counting objects:  47% (40/85)\u001b[K\rremote: Counting objects:  48% (41/85)\u001b[K\rremote: Counting objects:  49% (42/85)\u001b[K\rremote: Counting objects:  50% (43/85)\u001b[K\rremote: Counting objects:  51% (44/85)\u001b[K\rremote: Counting objects:  52% (45/85)\u001b[K\rremote: Counting objects:  54% (46/85)\u001b[K\rremote: Counting objects:  55% (47/85)\u001b[K\rremote: Counting objects:  56% (48/85)\u001b[K\rremote: Counting objects:  57% (49/85)\u001b[K\rremote: Counting objects:  58% (50/85)\u001b[K\rremote: Counting objects:  60% (51/85)\u001b[K\rremote: Counting objects:  61% (52/85)\u001b[K\rremote: Counting objects:  62% (53/85)\u001b[K\rremote: Counting objects:  63% (54/85)\u001b[K\rremote: Counting objects:  64% (55/85)\u001b[K\rremote: Counting objects:  65% (56/85)\u001b[K\rremote: Counting objects:  67% (57/85)\u001b[K\rremote: Counting objects:  68% (58/85)\u001b[K\rremote: Counting objects:  69% (59/85)\u001b[K\rremote: Counting objects:  70% (60/85)\u001b[K\rremote: Counting objects:  71% (61/85)\u001b[K\rremote: Counting objects:  72% (62/85)\u001b[K\rremote: Counting objects:  74% (63/85)\u001b[K\rremote: Counting objects:  75% (64/85)\u001b[K\rremote: Counting objects:  76% (65/85)\u001b[K\rremote: Counting objects:  77% (66/85)\u001b[K\rremote: Counting objects:  78% (67/85)\u001b[K\rremote: Counting objects:  80% (68/85)\u001b[K\rremote: Counting objects:  81% (69/85)\u001b[K\rremote: Counting objects:  82% (70/85)\u001b[K\rremote: Counting objects:  83% (71/85)\u001b[K\rremote: Counting objects:  84% (72/85)\u001b[K\rremote: Counting objects:  85% (73/85)\u001b[K\rremote: Counting objects:  87% (74/85)\u001b[K\rremote: Counting objects:  88% (75/85)\u001b[K\rremote: Counting objects:  89% (76/85)\u001b[K\rremote: Counting objects:  90% (77/85)\u001b[K\rremote: Counting objects:  91% (78/85)\u001b[K\rremote: Counting objects:  92% (79/85)\u001b[K\rremote: Counting objects:  94% (80/85)\u001b[K\rremote: Counting objects:  95% (81/85)\u001b[K\rremote: Counting objects:  96% (82/85)\u001b[K\rremote: Counting objects:  97% (83/85)\u001b[K\rremote: Counting objects:  98% (84/85)\u001b[K\rremote: Counting objects: 100% (85/85)\u001b[K\rremote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (79/79), done.\u001b[K\n",
            "remote: Total 296 (delta 15), reused 68 (delta 6), pack-reused 211\n",
            "Receiving objects: 100% (296/296), 47.12 MiB | 26.15 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncKi0KXyt4dx",
        "colab_type": "code",
        "outputId": "3f26d481-4894-4bd3-e6a5-4d16f950855a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd \"/content/Faces\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Faces\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5dN5N3itMEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fr_utils import *\n",
        "from inception_blocks_v2 import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ptQowico2Pi",
        "colab_type": "text"
      },
      "source": [
        "In Face Verification, you're given two images and you have to tell if they are of the same person. The simplest way to do this is to compare the two images pixel-by-pixel. If the distance between the raw images are less than a chosen threshold, it may be the same person!\n",
        "Of course, this algorithm performs really poorly, since the pixel values change dramatically due to variations in lighting, orientation of the person's face, even minor changes in head position, and so on.\n",
        "\n",
        "You'll see that rather than using the raw image, you can learn an encoding $f(img)$ so that element-wise comparisons of this encoding gives more accurate judgements as to whether two pictures are of the same person."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmP9PvQJpE3n",
        "colab_type": "text"
      },
      "source": [
        "#1 - Encoding face images into a 128-dimensional vector\n",
        "\n",
        "\n",
        "##1.1 - Using an ConvNet to compute encodings\n",
        "The FaceNet model takes a lot of data and a long time to train. So following common practice in applied deep learning settings, let's just load weights that someone else has already trained. The network architecture follows the Inception model from Szegedy et al.. We have provided an inception network implementation. You can look in the file inception_blocks.py to see how it is implemented.\n",
        "\n",
        "The key things you need to know are:\n",
        "\n",
        "This network uses 96x96 dimensional RGB images as its input. Specifically, inputs a face image (or batch of $m$ face images) as a tensor of shape $(m, n_C, n_H, n_W) = (m, 3, 96, 96)$\n",
        "It outputs a matrix of shape $(m, 128)$ that encodes each input face image into a 128-dimensional vector\n",
        "Run the cell below to create the model for face images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx5r42aGlk6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FRmodel = faceRecoModel(input_shape=(3, 96, 96))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CefSjXJnzspz",
        "colab_type": "text"
      },
      "source": [
        "Let's see how many parameters I have now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzQPFi-Jzw3Z",
        "colab_type": "code",
        "outputId": "dd071245-f859-4a2e-ade2-c6b121004ff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Total Params:\", FRmodel.count_params())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 3743280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlIpOOXS0B2k",
        "colab_type": "text"
      },
      "source": [
        "Not bad. So this means I have pictures and I learned an inception network, which will produce a 128 dimensional vector for each picture I have. Pictures from the same person in different situations should be closer than a reasonable threshold, and pics of different people will have a greater distance.\n",
        "\n",
        "To explain it, I pass 2 pictures through the network. Then I will compare the two outputs (distance, substraction, or whatever). And with this I decide if they are the same person or not.\n",
        "\n",
        "So if I have a picture A in my database. Then I get one of the same person, A', and another from a different person, B. My encoding will minimize the distance between A and A' and maximize the distance between A and B. The distance in 128 dimensions, careful with this. A is usually called anchor and this is usually called the triplet loss. It's in the paper and the notation is confusing to me, but writing it down helps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Etcu4cp6BIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
        "    \"\"\"\n",
        "    Obviously this function implements the triplet loss\n",
        "    \n",
        "    y_true are the true labels.\n",
        "    y_pred is a list with three objects\n",
        "        --- anchor: the encoding of the anchor images, shape = (None, 128)\n",
        "        --- positive: encodings of the positive images, shape = (None, 128)\n",
        "        --- negative: encodings of the negative images, shape = (None, 128)\n",
        "    \n",
        "    alpha is a margin parameter. Manually chosen.\n",
        "    \n",
        "    returns the loss value (real number)\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
        "    \n",
        "    #PArt 1: Distance from anchor to positive (last dimension, look at the shapes!)\n",
        "    \n",
        "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis = -1)\n",
        "    \n",
        "    #PArt 2: Distance from anchor to negative\n",
        "    \n",
        "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis = -1)\n",
        "    \n",
        "    # Substract these two distances and add alpha. I can use + or - because I'm dealing with real numbers, not tensors.\n",
        "    \n",
        "    basic_loss = pos_dist - neg_dist + alpha\n",
        "    \n",
        "    # Now take the maximum between 0 and basic_loss. If it's 0 I can't reduce more.\n",
        "    #Use 0.0 because if I use 0 it's an int, not a float. And basic_loss is a float    \n",
        "    loss = tf.reduce_sum(tf.maximum(0.0, basic_loss)) # Here I will take the first axis, I need to sum all the basic losses\n",
        "    \n",
        "    return loss\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcO05WM89QAR",
        "colab_type": "code",
        "outputId": "eebbfb4c-ca35-4cab-bd2a-516aa3adda9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with tf.Session() as test:\n",
        "    \n",
        "    tf.set_random_seed(1)\n",
        "    y_true = (None, None, None) # I don't need this in this cell\n",
        "    y_pred = (tf.random_normal([3, 128], mean=6, stddev=0.1, seed = 1),\n",
        "              tf.random_normal([3, 128], mean=1, stddev=1, seed = 1),\n",
        "              tf.random_normal([3, 128], mean=3, stddev=4, seed = 1))\n",
        "    \n",
        "    loss = triplet_loss(y_true, y_pred)\n",
        "    print(\"Loss = \" + str(loss.eval()))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss = 528.1427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHypV0mbBntu",
        "colab_type": "text"
      },
      "source": [
        "Obviously training this model takes a lot of time and resources, even in colab. So I will compile the model and load the weights (the weights take a couple of minutes to read)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJF3a1C3CY3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FRmodel.compile(optimizer = \"adam\", loss = triplet_loss, metrics = [\"accuracy\"])\n",
        "\n",
        "load_weights_from_FaceNet(FRmodel)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEk4tipkEG3u",
        "colab_type": "text"
      },
      "source": [
        "Now let's apply the model to some pictures. The following code takes a picture and produces an encoding and I just need to provide a dictionary to be filled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBJgDeoQEami",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "database = {}\n",
        "database[\"danielle\"] = img_to_encoding(\"images/danielle.png\", FRmodel)\n",
        "database[\"younes\"] = img_to_encoding(\"images/younes.jpg\", FRmodel)\n",
        "database[\"tian\"] = img_to_encoding(\"images/tian.jpg\", FRmodel)\n",
        "database[\"andrew\"] = img_to_encoding(\"images/andrew.jpg\", FRmodel)\n",
        "database[\"kian\"] = img_to_encoding(\"images/kian.jpg\", FRmodel)\n",
        "database[\"dan\"] = img_to_encoding(\"images/dan.jpg\", FRmodel)\n",
        "database[\"sebastiano\"] = img_to_encoding(\"images/sebastiano.jpg\", FRmodel)\n",
        "database[\"bertrand\"] = img_to_encoding(\"images/bertrand.jpg\", FRmodel)\n",
        "database[\"kevin\"] = img_to_encoding(\"images/kevin.jpg\", FRmodel)\n",
        "database[\"felix\"] = img_to_encoding(\"images/felix.jpg\", FRmodel)\n",
        "database[\"benoit\"] = img_to_encoding(\"images/benoit.jpg\", FRmodel)\n",
        "database[\"arnaud\"] = img_to_encoding(\"images/arnaud.jpg\", FRmodel)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmGiu0v_WwD1",
        "colab_type": "text"
      },
      "source": [
        "Now, if all these people are the members, I can go and compare anyone who shows up by taking his picture and comparing with my encoding. This is done with the next function. I consider that he swiped his card or code or whatever, so I'm checking him against his picture. Not face ID but ID verification (1:1 problem, not 1:n)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TwGFUmqXA-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def verify(image_path, identity, model):\n",
        "    \n",
        "    \"\"\"\n",
        "    image path: that's what it looks like.\n",
        "    Identity: a string with the name of someone, contained in database.\n",
        "    model is my encoding of the given person, FRmodel\n",
        "    \n",
        "    output\n",
        "    dist: distance between the encoding and the person at the door. should be closer than threshold.\n",
        "    door_open: True or False.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    # First compute the encoding of the person trying to enter\n",
        "    encoding = img_to_encoding(image_path, model)\n",
        "    \n",
        "    #Now compute the distance (L2). Identity here is not a string, it's a variable of type string!! Otherwise it would be constant\n",
        "    dist = np.linalg.norm(encoding - database[identity])\n",
        "    \n",
        "    # If dist is less than 0.7, open, otherwise close\n",
        "    \n",
        "    if dist < 0.7:\n",
        "        door_open = True\n",
        "    else:\n",
        "        door_open = False\n",
        "    \n",
        "    return dist, door_open"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPT8h0B7Y9sl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a9ba425c-f516-4d88-9c1b-65bc762fd680"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Faces\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJV2DPUeYnek",
        "colab_type": "text"
      },
      "source": [
        "Now I will verify with someone. It should be true because I chose it to be True. I'll copy the same code with a different identity to check in the second cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usBIdH3BYqH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c9425986-efdd-41ce-daf3-0d620415f583"
      },
      "source": [
        "verify(\"/content/Faces/images/camera_0.jpg\", \"younes\", FRmodel)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.67100644, True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dJ5qHf7Z_lz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "edb4237f-daae-4a9b-eac5-f9b0ebb99a24"
      },
      "source": [
        "verify(\"/content/Faces/images/camera_0.jpg\", \"danielle\", FRmodel)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.2086712, False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3awEm3Ba8A7",
        "colab_type": "text"
      },
      "source": [
        "Now I want to do my face recognition, not ID. To avoid using the ID card or code or whatever. The idea is that someone comes, I take their picture and with this witchcraft I open or not. In practice:\n",
        "\n",
        "\n",
        "*   Encode picture\n",
        "*   Calculate the distance by looping over my database. Save the minimum and if it's lower than the threshold I open.\n",
        "\n",
        "Simple, right? But notice that I don't input an identity, I input all the database to compare the pic to everyone.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o24x4ZTbg61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def who_is_it(image_path, database, model):\n",
        "    \"\"\"\n",
        "    image path is the picture\n",
        "    database is the database of encodings + names\n",
        "    model is the keras model that encodes, as before\n",
        "    \n",
        "    output:\n",
        "    min_dist, the minimum distance\n",
        "    identity a string, the prediction of the person's name\n",
        "    \n",
        "    \"\"\"\n",
        "    # Again I take the picture and encoding, as before\n",
        "    \n",
        "    encoding_who = img_to_encoding(image_path, model)\n",
        "    \n",
        "    # I initialize min_dist at a huge level, 1e6 is ok\n",
        "    \n",
        "    min_dist = 1e6\n",
        "    \n",
        "    # Loop over the database dictionary\n",
        "    \n",
        "    for (name, db_encod) in database.items():\n",
        "        \n",
        "        #compute distance\n",
        "        dist = np.linalg.norm(encoding_who - db_encod)\n",
        "        \n",
        "        # See if it's minimum or not.\n",
        "        if dist < min_dist:\n",
        "            min_dist = dist\n",
        "            identity = name\n",
        "    # And as before, I see if I open once I have found the minimum distance\n",
        "    \n",
        "    if min_dist < 0.7:\n",
        "        print(\"It's \", identity)\n",
        "    else:\n",
        "        print(\"I don't know you, go somewhere else\")\n",
        "    \n",
        "    \n",
        "    return min_dist, identity\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVOwYlJtdiye",
        "colab_type": "text"
      },
      "source": [
        "Now I check with the camera_0 and 1 pictures. If I input some other picture it should reject it, but I won't do it because resizing the image is a hassle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YNQfFGEdt59",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "544ec90f-2883-45c8-e987-74bf3a4ddb36"
      },
      "source": [
        "who_is_it(\"/content/Faces/images/camera_0.jpg\", database, FRmodel)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It's  younes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.67100644, 'younes')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QDYBZ6iezCi",
        "colab_type": "text"
      },
      "source": [
        "Now this was really useful. I got to understand how it works much better. Now some ways to improve the model.\n",
        "\n",
        "* Put more images of each person (under different lighting conditions, taken on different days, etc.) into the database. Then given a new image, compare the new face to multiple pictures of the person. This would increase accuracy.\n",
        "* Crop the images to just contain the face, and less of the \"border\" region around the face. This preprocessing removes some of the irrelevant pixels around the face, and also makes the algorithm more robust.\n",
        "* Face verification solves an easier 1:1 matching problem; face recognition addresses a harder 1:K matching problem.\n",
        "* The triplet loss is an effective loss function for training a neural network to learn an encoding of a face image.\n",
        "* The same encoding can be used for verification and recognition. Measuring distances between two images' encodings allows you to determine whether they are pictures of the same person."
      ]
    }
  ]
}